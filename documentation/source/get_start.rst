.. _getting_start:

#############################
Getting Start with MetaDrive
#############################

Tryout MetaDrive with one line
###############################

We provide a script to let you try out MetaDrive by keyboard immediately after installation! Please run::

    # Make sure current folder does not have a sub-folder named metadrive
    python -m metadrive.examples.drive_in_single_agent_env

In the same script, you can even experience an "auto-drive" journey carried out by our pre-trained RL agent. Press `T` in the main window will kick-off this.
You can also press `H` to visit the helper information on other shortcuts.


To enjoy the process of generating map through our Procedural Generation (PG) algorithm, please run this script::

    python -m metadrive.examples.procedural_generation


You can also draw multiple maps generated by PG in the top-down view via running::

    python -m metadrive.examples.draw_maps


Besides, you can verify the efficiency of MetaDrive via running::

    python -m metadrive.examples.profile_metadrive


As we will discuss in `Environments <rl_environments.html>`_, MetaDrive provides three sets of RL environments: the generalization environments, the real-world environments, the Safe RL environments and the Multi-agent RL environments.
We provide the examples for those suites as follow:

.. code-block::

    # Make sure current folder does not have a sub-folder named metadrive

    # ===== Generalization Environments =====
    python -m metadrive.examples.drive_in_single_agent_env

    # ===== Real-world Environments =====
    python -m metadrive.examples.drive_in_real_env

    # ===== Safe RL Environments =====
    python -m metadrive.examples.drive_in_safe_metadrive_env

    # ===== Multi-agent Environments =====
    # Options for --env: roundabout, intersection, tollgate, bottleneck, parkinglot, pgma
    python -m metadrive.examples.drive_in_multi_agent_env --env pgma



Using MetaDrive in Your Code
#############################

The usage of MetaDrive is as same as other **gym** environments.
Almost all decision making algorithms are compatible with MetaDrive, as long as they are compatible with OpenAI gym.
The following scripts is a minimal example for instantiating a MetaDrive environment instance.

.. code-block:: python

    from metadrive.envs.metadrive_env import MetaDriveEnv
    import gymnasium as gym

    env = MetaDriveEnv(config={"use_render": True})
    obs, info = env.reset()
    for i in range(1000):
        obs, reward, terminated, truncated, info = env.step(env.action_space.sample())
        if terminated or truncated:
            env.reset()
    env.close()

If your training framework is under the support of openai.gym instead of gymnasium, you can wrap the environment to make
it compatible with the training framework.

.. code-block:: python

    from metadrive.envs.metadrive_env import MetaDriveEnv
    import gymnasium as gym
    from metadrive.envs.gym_wrapper import createGymWrapper # import the wrapper

    env = createGymWrapper(MetaDriveEnv)(config={"use_render": True}) # wrap the environment
    obs = env.reset()
    for i in range(1000):
        obs, reward, done, info = env.step(env.action_space.sample()) # the return value contains no truncate
        if done:
            env.reset()
    env.close()


.. Note:: Please note that each process should only have one single MetaDrive instance due to the limit of the underlying simulation engine.
    Thus the parallelization of training environment should be in process-level instead of thread-level.

You can also try out our example of using RLLib/stable-baselines3 to train RL policies in `Training <training.html>`_